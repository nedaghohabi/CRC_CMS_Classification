# Image-Based CMS Classification of Colorectal Cancer (ResNet-34)

This repository contains code and configs for our study:
**â€œImage-Based Consensus Molecular Subtype Classification of Colorectal Cancer using Deep Learning.â€**

We train a ResNet-34 CNN on tumor tiles from TCGA COAD/READ histopathology whole-slide images (WSIs) to predict **CMS1â€“CMS4** subtypes. Labels are derived from RNA-seq using the **CMSclassifier** package. The repo includes:
- Training & evaluation on COAD/READ (10-fold CV + held-out test)
- Tile-level inference + **sample-level majority voting** for **unclassified** cases
- Pointers to external tools for **WSI preprocessing** and **transcriptomic CMS label generation**

> âš ï¸ **Note**: This code is research-grade. External validation is required before any clinical use.

---

## Contents
- [Overview](#overview)
- [Data & Preprocessing](#data--preprocessing)
- [CMS Label Generation (RNA-seq)](#cms-label-generation-rna-seq)
- [Environment](#environment)
- [Repository Structure](#repository-structure)
- [Configuration](#configuration)
- [Training & Evaluation](#training--evaluation)
- [Prediction for Unclassified Samples](#prediction-for-unclassified-samples)
- [Results Artifacts](#results-artifacts)
- [Citations](#citations)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## Overview

- **Backbone**: ResNet-34 (ImageNet-pretrained), first conv optionally replaced with 5Ã—5 (stride=2, pad=2)
- **Input**: tiles **224Ã—224** randomly cropped from **512Ã—512** tumor patches
- **Cohorts**: TCGA **COAD** and **READ**, trained **separately** with identical settings
- **Split**: Default **10-fold CV** on tiles + **10% held-out test** (configurable)
- **Metrics**: Accuracy, Precision/Recall/F1, **ROC-AUC** (per-class, micro, macro), Confusion Matrix
- **Post-hoc**: Predict **unclassified** samples via tile-level thresholds + majority voting

---

## Data & Preprocessing

WSIs and RNA-seq come from **TCGA**.

- **WSI Source**: TCGA (GDC)  
  - Portal: https://portal.gdc.cancer.gov/
  - Download format: **.svs**
- **Tumor ROI annotation & tiling**: We used QuPath scripting to annotate tumor regions (pathologist-supervised), tessellate into **512Ã—512** tiles at 20Ã—, remove non-informative tiles, and export JPGs.

ğŸ‘‰ **Preprocessing helper repo** (annotation + tiling scripts):  
**[ADD_LINK_HERE to the external preprocessing repository]**

Expected folder layout after preprocessing (example):



> Tip: We kept tiles with <50% background and discarded out-of-focus/artifact tiles. Update thresholds in your preprocessing repo as needed.

---

## CMS Label Generation (RNA-seq)

Labels are derived using **CMSclassifier** on TCGA RNA-seq (FPKM-UQ), following the **RF + SSP consensus** approach (match required; no extra cut-offs).

ğŸ‘‰ **CMS label generation helper repo/script**:  
**[ADD_LINK_HERE to the external CMSclassifier R repository/script]**

We used:
- **TCGAbiolinks** to retrieve RNA-seq & clinical data
- **edgeR / limma** default normalization on FPKM-UQ
- **CMSclassifier** (RF posterior â‰¥ 0.5) and **SSP** after row-centering
- Final label retained only if **RF == SSP**; otherwise **unclassified**


---

## Environment

- **Python**: 3.9+
- **PyTorch**: 2.x (CUDA 11.8 recommended)
- **R** (for CMSclassifier; run in the external repo)
- **QuPath** 0.3.2 (if you reproduce annotations/tiling)

Create a conda env (example):
```bash
conda create -n cms-imaging python=3.9 -y
conda activate cms-imaging
pip install -r requirements.txt

.
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ coad.yaml
â”‚   â””â”€â”€ read.yaml
â”œâ”€â”€ data/                      # (not tracked) tiles organized by CMS class
â”œâ”€â”€ labels/
â”‚   â”œâ”€â”€ coad_labels.csv
â”‚   â””â”€â”€ read_labels.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ datasets.py            # Tile dataset, transforms, samplers
â”‚   â”œâ”€â”€ model.py               # ResNet-34 builder (optionally 5Ã—5 first conv)
â”‚   â”œâ”€â”€ train_eval.py          # k-fold CV + held-out test
â”‚   â”œâ”€â”€ predict_unclassified.py# tile-level inference + majority voting
â”‚   â”œâ”€â”€ metrics.py             # AUC, reports, confusions
â”‚   â””â”€â”€ utils.py               # logging, seed, checkpoint IO
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ coad/
â”‚   â”‚   â”œâ”€â”€ cv/                # per-fold metrics, ROC, confusion
â”‚   â”‚   â””â”€â”€ test/              # held-out test metrics & plots
â”‚   â””â”€â”€ read/
â”‚       â”œâ”€â”€ cv/
â”‚       â””â”€â”€ test/
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ figures/
â”‚       â””â”€â”€ workflow_main_1.png
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE


requirements.txt


torch
torchvision
torchaudio
numpy
pandas
scikit-learn
scipy
matplotlib
tqdm
pyyaml
opencv-python


configs/coad.yaml


dataset_name: COAD
data_root: ./data/COAD/tiles
labels_csv: ./labels/coad_labels.csv

image_size: 224
tile_size: 512
batch_size: 16
num_workers: 8
epochs: 5

model:
  backbone: resnet34
  imagenet_pretrained: true
  first_conv_5x5: true
  num_classes: 4

optimizer:
  name: adam
  lr: 1e-4
  weight_decay: 5e-4
  amsgrad: true

cv:
  folds: 10
  split_level: tile   # tile | patient (set to patient if you want stricter splitting)
  test_fraction: 0.10
  stratified: true

augment:
  hflip_prob: 0.5
  vflip_prob: 0.5
  rotate_deg: 45

inference:
  prob_threshold: 0.5
  voting: majority


## training--evaluation

python -m src.train_eval --config ./configs/coad.yaml --out ./results/coad

Outputs:
Per-fold metrics: accuracy, precision/recall/F1, ROC-AUC (per-class, micro, macro)
Plots: ROC curves & confusion matrices (saved under results/<cohort>/cv/)
Best fold checkpoint â†’ evaluated on held-out test set (saved under results/<cohort>/test/)
